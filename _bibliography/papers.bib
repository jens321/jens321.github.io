---
---

@inproceedings{todo,
  title={Gradient-based Analysis of NLP Models is Manipulable},
  author={Junlin Wang* and Jens Tuyls* and Eric Wallace and Sameer Singh},
  Booktitle={Findings of Empirical Methods in Natural Language Processing},
  year={2020},
  arxiv={2010.05419},
  abstract={Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, their faithfulness. In this paper, however, we demonstrate that the gradients of a model are easily manipulable, and thus bring into question the reliability of gradient-based analyses. In particular, we merge the layers of a target model with a Facade that overwhelms the gradients without affecting the predictions. This Facade can be trained to have gradients that are misleading and irrelevant to the task, such as focusing only on the stop words in the input. On a variety of NLP tasks (text classification, NLI, and QA), we show that our method can manipulate numerous gradient-based analysis techniques: saliency maps, input reduction, and adversarial perturbations all identify unimportant or targeted tokens as being highly important.},
  website={https://ucinlp.github.io/facade/}
}

@inproceedings{Mooers2020GenerativeMF,
  title={Generative Modeling for Atmospheric Convection},
  author={G. Mooers and Jens Tuyls and S. Mandt and M. Pritchard and Tom Beucler},
  Booktitle={International Conference on Climate Informatics},
  year={2020},
  arxiv={2007.01444},
  abstract={To improve climate modeling, we need a better understanding of multi-scale atmospheric dynamics – the relationship between large scale environment and small-scale storm formation, morphology and propagation – as well as superior stochastic parameterization of convective organization. We analyze raw output from ∼ 6 million instances of explicitly simulated convection spanning all global geographic regimes of convection in the tropics, focusing on the vertical velocities extracted every 15 minutes from ∼ 4 hundred thousands separate instances of a storm-permitting moist turbulence model embedded within a multi-scale global model of the atmosphere.
Generative modeling techniques applied on high-resolution climate data for representation learning hold the potential to drive next-generation parameterization and breakthroughs in understanding of convection and storm development. To that end, we design and implement a specialized Variational Autoencoder (VAE) to perform structural replication, dimensionality reduction and clustering on these cloud-resolving vertical velocity outputs. Our VAE reproduces the structure of disparate classes of convection, successfully capturing both their magnitude and variances. This VAE thus provides a novel way to perform unsupervised grouping of convective organization in multi-scale simulations of the atmosphere in a physically sensible manner. The success of our VAE in structural emulation, learning physical meaning in convective transitions and anomalous vertical velocity field detection may help set the stage for developing generative models for stochastic parameterization that might one day replace explicit convection calculations.}
}

@inproceedings{Kerrigan2020DifferentiallyPL,
  title={Differentially Private Language Models Benefit from Public Pre-training},
  author={Gavin Kerrigan and Dylan Slack and Jens Tuyls},
  Booktitle={EMNLP PrivateNLP Workshop},
  year={2020},
  arxiv={2009.05886},
  abstract={Language modeling is a keystone task in natu- ral language processing. When training a lan- guage model on sensitive information, differ- ential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultane- ously high-quality and privacy preserving by tuning a public base model on a private cor- pus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible.}
}

@inproceedings{Wallace2019AllenNLP,
  title={{AllenNLP Interpret}: A Framework for Explaining Predictions of {NLP} Models},
  author={Eric Wallace and Jens Tuyls and Junlin Wang and Sanjay Subramanian and Matt Gardner and Sameer Singh},
  Booktitle={Empirical Methods in Natural Language Processing},
  year={2019},
  arxiv={1909.09251},
  website={https://allennlp.org/interpret},
  poster={InterpretPoster.pdf},
  abstract={Neural NLP models are increasingly accurate but are imperfect and opaque — they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpretation methods ameliorate this opacity by providing explanations for specific model predictions. Unfortunately, existing interpretation codebases make it difficult to apply these methods to new models and tasks, which hinders adoption for practitioners and burdens interpretability researchers. We introduce AllenNLP Interpret, a flexible framework for interpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit’s flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension us- ing BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp.org/interpret.},
  award={Best Demo Award}
}

@inproceedings{tuyls2022multi,
  title={Multi-Stage Episodic Control for Strategic Exploration in Text Games},
  author={Jens Tuyls and Shunyu Yao and Sham Kakade and Karthik Narasimhan},
  Booktitle={International Conference on Learning Representations},
  year={2022},
  abstract={Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global de- cisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by 24\% and 10\% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods.},
  arxiv={2201.01251},
  code={https://github.com/princeton-nlp/XTX},
  award={Spotlight}
}

@inproceedings{tuyls2023scaling,
  title={Scaling Laws for Imitation Learning in Single-Agent Games},
  author={Jens Tuyls and Dhruv Madeka and Kari Torkkola and Dean Foster and Karthik Narasimhan and Sham Kakade},
  Booktitle={Transactions on Machine Learning Research},
  year={2024},
  abstract={Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, many works find it is often unable to fully recover the underlying expert behavior (Wen et al., 2020; Jacob et al., 2022), even in constrained environments like single-agent games (De Haan et al., 2019; Hambro et al., 2022b). However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) (Kaplan et al., 2020; Hoffmann et al., 2022) where “scaling up” has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring simi- lar improvements in the imitation learning setting for single-agent games. We first demonstrate our findings on a variety of Atari games, and there- after focus on the extremely challenging game of NetHack. In all games, we find that IL loss and mean return scale smoothly with the compute bud- get (FLOPs) and are strongly correlated, resulting in power laws for training compute-optimal IL agents. Finally, we forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by 1.5x in all settings. Our work both demonstrates the scaling behavior of imitation learning in a variety of single-agent games, as well as the viability of scaling up cur- rent approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems.},
  arxiv={2307.09423},
  code={https://github.com/princeton-nlp/il-scaling-in-games}
}

@inproceedings{zhang2024languageguided,
  title={Language-guided World Models: A Model-based Approach to AI Control},
  author={Alex L Zhang* and Khanh Xuan Nguyen* and Jens Tuyls and Albert Lin and Karthik R Narasimhan},
  booktitle={ACL SpLU-RoboNLP Workshop},
  year={2024},
  abstract={This paper introduces the concept of Language- Guided World Models (LWMs)—probabilistic models that can simulate environments by reading texts. Agents equipped with these models provide humans with more extensive and efficient control, allowing them to simultaneously alter agent behaviors in multiple tasks via natural verbal communication. In this work, we take initial steps in developing robust LWMs that can generalize to compositionally novel language descriptions. We design a challenging world modeling benchmark based on the game of MESSENGER (Hanjie et al., 2021), featuring evaluation settings that require varying degrees of compositional generalization. Our exper- iments reveal the lack of generalizability of the state-of-the-art Transformer model, as it offers marginal improvements in simulation qual- ity over a no-text baseline. We devise a more robust model by fusing the Transformer with the EMMA attention mechanism (Hanjie et al., 2021). Our model substantially outperforms the Transformer and approaches the performance of a model with an oracle semantic parsing and grounding capability. To demonstrate the practicality of this model in improving AI safety and transparency, we simulate a scenario in which the model enables an agent to present plans to a human before execution, and to re- vise plans based on their language feedback.},
  arxiv={2402.01695},
  code={https://github.com/princeton-nlp/lwm/},
  award={Oral}
}

@inproceedings{tuyls2025representation,
  title={Representation-Based Exploration for Language Models: From Test-Time to Post-Training},
  author={Jens Tuyls and Dylan J. Foster and Akshay Krishnamurthy and Jordan T. Ash},
  booktitle={International Conference on Learning Representations},
  abstract={Reinforcement learning (RL) promises to expand the capabilities of language models, but it is unclear if current RL techniques promote the discovery of novel behaviors, or simply sharpen those already present in the base model. In this paper, we investigate the value of deliberate exploration -- explicitly incentivizing the model to discover novel and diverse behaviors -- and aim to understand how the knowledge in pre-trained models can guide this search. Our main finding is that exploration with a simple, principled, representation-based bonus derived from the pre-trained language model's hidden states significantly improves diversity and pass@k rates -- both for post-training, and in a novel inference-time scaling setting we introduce. For inference-time, exploration with representation-based diversity improves efficiency, consistently improving pass@k rates across a variety of models and reasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50% improvement in verifier efficiency on almost all tasks. For post-training, we show that integrating this exploration strategy into an RL pipeline improves reasoning performance over that of the initial model and over standard RL post-training. For example, on AIME 2024, our post-trained Qwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model, demonstrating a 3x improvement in test-time sample efficiency. Overall, our findings suggest that deliberate exploration -- with the right notion of diversity -- is a practical path toward discovery of new behaviors beyond sharpening.},
  arxiv={2510.11686},
  year={2026},
  code={https://github.com/verl-project/verl-recipe/tree/main/rep_exp}
}

@inproceedings{zheng2024misl,
  title={Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning},
  author={Chongyi Zheng* and Jens Tuyls* and Joanne Peng and Benjamin Eysenbach},
  booktitle={International Conference on Learning Representations},
  year={2025},
  abstract={Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL). Our analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA.},
  code={https://github.com/Princeton-RL/contrastive-successor-features},
  arxiv={2412.08021},
  award={Oral}
}

@inproceedings{ellis2025training,
  title={Training LLM Agents to Empower Humans},
  author={Evan Ellis and Vivek Myers and Jens Tuyls and Sergey Levine and Anca Dragan and Benjamin Eysenbach},
  booktitle={Deep Learning for Code Workshop (NeurIPS)},
  year={2025},
  abstract={Assistive agents should not only take actions on behalf of a human, but also step out of the way and cede control when there are important decisions to be made. However, current methods for building assistive agents, whether via mimicking expert humans or via RL finetuning on an inferred reward, often encourage agents to complete tasks on their own rather than truly assisting the human attain their objectives. Additionally, these methods often require costly explicit human feedback to provide a training signal. We propose a new approach to tuning assistive language models based on maximizing the human's empowerment, their ability to effect desired changes in the environment. Our empowerment-maximizing method, Empower, only requires offline text data, providing a self-supervised method for fine-tuning language models to better assist humans. To study the efficacy of our approach, we conducted an 18-person user study comparing our empowerment assistant with a strong baseline. Participants preferred our assistant 78% of the time (p=0.015), with a 31% higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a new environment for evaluating multi-turn code assistance using simulated humans. Using this environment, we show that agents trained with Empower increase the success rate of a simulated human programmer on challenging coding questions by an average of 192% over an SFT baseline. With this empowerment objective, we provide a framework for useful aligned AI agents at scale using only offline data without the need for any additional human feedback or verifiable rewards.},
  code={https://github.com/festusev/codegen_empowerment/tree/main},
  arxiv={2510.13709},
  award={Best Paper},
}

@article{paglieri2025learning,
  title={Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents},
  author={Davide Paglieri and Bart{\l}omiej Cupia{\l} and Jonathan Cook and Ulyana Piterbarg and Jens Tuyls and Edward Grefenstette and Jakob Nicolaus Foerster and Jack Parker-Holder and Tim Rockt{\"a}schel},
  abstract={Training large language models (LLMs) to reason via reinforcement learning (RL) significantly improves their problem-solving capabilities. In agentic settings, existing methods like ReAct prompt LLMs to explicitly plan before every action; however, we demonstrate that always planning is computationally expensive and degrades performance on long-horizon tasks, while never planning further limits performance. To address this, we introduce a conceptual framework formalizing dynamic planning for LLM agents, enabling them to flexibly decide when to allocate test-time compute for planning. We propose a simple two-stage training pipeline: (1) supervised fine-tuning on diverse synthetic data to prime models for dynamic planning, and (2) RL to refine this capability in long-horizon environments. Experiments on the Crafter environment show that dynamic planning agents trained with this approach are more sample-efficient and consistently achieve more complex objectives. Additionally, we demonstrate that these agents can be effectively steered by human-written plans, surpassing their independent capabilities. To our knowledge, this work is the first to explore training LLM agents for dynamic test-time compute allocation in sequential decision-making tasks, paving the way for more efficient, adaptive, and controllable agentic systems.},
  arxiv={2509.03581},
  year={2025}
}