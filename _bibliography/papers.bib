---
---

@inproceedings{todo,
  title={Gradient-based Analysis of NLP Models is Manipulable},
  author={Junlin Wang and Jens Tuyls and Eric Wallace and Sameer Singh},
  Booktitle={Findings of Empirical Methods in Natural Language Processing},
  year={2020}
}

@inproceedings{Mooers2020GenerativeMF,
  title={Generative Modeling for Atmospheric Convection},
  author={G. Mooers and Jens Tuyls and S. Mandt and M. Pritchard and Tom Beucler},
  Booktitle={International Conference on Climate Informatics},
  year={2020},
  arxiv={https://arxiv.org/abs/2007.01444},
  abstract={To improve climate modeling, we need a better understanding of multi-scale atmospheric dynamics – the relationship between large scale environment and small-scale storm formation, morphology and propagation – as well as superior stochastic parameterization of convective organization. We analyze raw output from ∼ 6 million instances of explicitly simulated convection spanning all global geographic regimes of convection in the tropics, focusing on the vertical velocities extracted every 15 minutes from ∼ 4 hundred thousands separate instances of a storm-permitting moist turbulence model embedded within a multi-scale global model of the atmosphere.
Generative modeling techniques applied on high-resolution climate data for representation learning hold the potential to drive next-generation parameterization and breakthroughs in understanding of convection and storm development. To that end, we design and implement a specialized Variational Autoencoder (VAE) to perform structural replication, dimensionality reduction and clustering on these cloud-resolving vertical velocity outputs. Our VAE reproduces the structure of disparate classes of convection, successfully capturing both their magnitude and variances. This VAE thus provides a novel way to perform unsupervised grouping of convective organization in multi-scale simulations of the atmosphere in a physically sensible manner. The success of our VAE in structural emulation, learning physical meaning in convective transitions and anomalous vertical velocity field detection may help set the stage for developing generative models for stochastic parameterization that might one day replace explicit convection calculations.}
}

@inproceedings{Kerrigan2020DifferentiallyPL,
  title={Differentially Private Language Models Benefit from Public Pre-training},
  author={Gavin Kerrigan and Dylan Slack and Jens Tuyls},
  Booktitle={EMNLP PrivateNLP Workshop},
  year={2020},
  arxiv={https://arxiv.org/abs/2009.05886},
  abstract={Language modeling is a keystone task in natu- ral language processing. When training a lan- guage model on sensitive information, differ- ential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultane- ously high-quality and privacy preserving by tuning a public base model on a private cor- pus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible.}
}

@inproceedings{Wallace2019AllenNLP,
  title={{AllenNLP Interpret}: A Framework for Explaining Predictions of {NLP} Models},
  author={Eric Wallace and Jens Tuyls and Junlin Wang and Sanjay Subramanian and Matt Gardner and Sameer Singh},
  Booktitle={Empirical Methods in Natural Language Processing},
  year={2019},
  arxiv={https://arxiv.org/abs/1909.09251},
  website={https://allennlp.org/interpret},
  poster={InterpretPoster.pdf},
  abstract={Neural NLP models are increasingly accurate but are imperfect and opaque — they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpretation methods ameliorate this opacity by providing explanations for specific model predictions. Unfortunately, existing interpretation codebases make it difficult to apply these methods to new models and tasks, which hinders adoption for practitioners and burdens interpretability researchers. We introduce AllenNLP Interpret, a flexible framework for interpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit’s flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension us- ing BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp.org/interpret.}
}